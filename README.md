# "Детекция объектов в видеопотоке с помощью YOLO: анализ производительности и оптимизация."

## Аннотация

В данной работе проведу исследование производительности YOLO при детекции объектов в видеопотоке.

**Актуальность**: Детекция объектов в видеопотоке - важная задача для приложений компьютерного зрения в реальном времени, таких как видеонаблюдение и автономное вождение.

## Задача обнаружения объектов

Обнаружение объектов — это задача компьютерного зрения, которая заключается в идентификации и поиске объектов предопределенных классов на входных изображениях.

С развитием глубокого обучения обнаружение объектов достигло весьма многообещающих результатов. Существует несколько алгоритмов и моделей, таких как R-CNN, Faster R-CNN, YOLO и SSD, 
которые были разработаны для обнаружения объектов. Эти алгоритмы и модели используются в различных приложениях, таких как беспилотные автомобили, системы наблюдения и отслеживание объектов (object tracking).

### Region-based CNN

Существует целый класс сверточных нейронных сетей, основанный на регионах, областях или ограничивающих рамок.

Основная идея таких CNN заключается в том, чтобы сначала сгенерировать набор потенциальных регионов или «предложений регионов» в пределах изображения или видео, а затем использовать CNN для классификации 
каждого региона как содержащего объект или нет. Регионы, которые классифицируются как содержащие объект, затем дополнительно обрабатываются для уточнения местоположения объекта и классификации его в определенный класс. 
Эта концепция также известна как двухкадровое обнаружение объектов.

Одним из самых известных примеров CNN на основе регионов является R-CNN (Regional CNN) и его варианты, такие как Fast R-CNN, Faster R-CNN и Mask R-CNN, которые были предложены для повышения скорости и точности процесса обнаружения объектов. 
Эти методы используют комбинацию выборочного поиска или других методов предложения регионов для генерации регионов-кандидатов и CNN для классификации и уточнения ‌регионов, содержащих объекты.

![Структура RCNN](https://github.com/comedian73/Yolo_fish_detekt/blob/main/foto_yolo_/RCNN_structure.png)
**Рис. 1. Структура RCNN**

Подробнее со структурой можно ознакомиться в оригинальной статье [Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497). Данная статья отражает состояние отрасли по CV на конец 2015 года. Именно в этом году вышла первая версия модели YOLO, о которой пойдет речь ниже.

Несмотря на то, что Region-based CNN могут решать задачу обнаружения объектов с относительно удовлетворительной точностью, у них есть несколько недостатков: высокая вычислительная стоимость, неэффективность, ограниченное обобщение и отсутствие контекста. 
Они вычислительно дороги и неэффективны, что делает их непрактичными для обнаружения объектов в реальном времени на маломощных устройствах. Кроме того, они требуют больших маркированных наборов данных для обучения, получение которых может быть дорогостоящим и трудоемким. 
Эти факторы делают их мало подходящими для применения в задачах компьютерного зрения в реальном времени.

### Метрики обнаружения объектов

Для оценки эффективности моделей обнаружения объектов обычно используется несколько показателей:

1. **Mean Average Precision (mAP)**. Точность модели определяется как количество истинно положительных результатов, деленное на количество истинно положительных результатов плюс ложноположительных результатов.
   Метрика mAP учитывает как точность, так и полноту модели и рассчитывается как среднее значение средней точности для каждого класса. Более высокое значение mAP указывает на лучшую производительность модели.

3. **Average Precision (AP)**. Это мера точности модели на разных уровнях полноты. Точность определяется как количество истинно положительных результатов, деленное на количество истинно положительных результатов плюс ложноположительных.
   Полнота определяется как количество истинно положительных результатов, деленное на количество истинно положительных результатов плюс ложноотрицательных. AP рассчитывается как площадь под кривой точности-полноты. Более высокое значение AP указывает на лучшую производительность модели.

5. **Intersection over Union (IoU)**. Это мера перекрытия между предсказанным ограничивающим прямоугольником и ограничивающим прямоугольником истинного значения.
   Она вычисляется путем деления площади пересечения двух прямоугольников на площадь их объединения. Более высокое значение IoU указывает на лучшее соответствие между предсказанным и ограничивающим прямоугольниками истинного значения.

![IoU](https://github.com/comedian73/Yolo_fish_detekt/blob/main/foto_yolo_/IoU.png)

### Как считать IoU?

Пусть рамки представленны набором координат $X$ и $Y$, где:

$$ X = (A_1, B_1, C_1, D_1)$$

$$ Y = (A_2, B_2, C_2, D_2)$$

Определим $A_{inter}$ и $B_{inter}$ как координаты верхнего левого угла ограничивающего прямоугольника, в то время как $C_{inter}$ и $D_{inter}$ - координаты нижнего правого угла ограничивающего прямоугольника.

Таким образом:

$$ A_{inter} = max(A_1, A_2)$$

$$ B_{inter} = max(B_1, B_2)$$

$$ C_{inter} = min(C_1, C_2)$$

$$ D_{inter} = min(D_1, D_2)$$

При этом, если значение $(C_{inter}<A_{inter})$ или $(D_{inter}<B_{inter})$, то пересечение между двумя прямоугольниками равно нулю и $$IoU = 0$$.

## Семейство моделей YOLO

Семейство моделей «You Only Look Once» или YOLO - это серия моделей сквозного глубокого обучения, разработанных Джозефом Редмоном и соавторами для обнаружения объектов в реальном времени. Она была впервые представлена в статье 2015 года под названием Ты смотришь только раз: унифицированное обнаружение объектов в реальном времени. 
Это одноступенчатый детектор объектов, который использует сверточную нейронную сеть (CNN) для прогнозирования ограничивающих рамок и вероятностей классов объектов на входных изображениях. Впервые YOLO был реализован с использованием фреймворка Darknet.

Алгоритм YOLO делит входное изображение на сетку ячеек, и для каждой ячейки он предсказывает вероятность присутствия объекта и координаты ограничивающей рамки объекта. 
Он также предсказывает класс объекта. В отличие от двухступенчатых детекторов объектов, таких как R-CNN и его варианты, YOLO обрабатывает все изображение за один проход, что делает его более быстрым и эффективным.

YOLO был разработан в нескольких версиях, начиная с YOLOv1 и по YOLOv11! Все очень быстро меняется, поэтому следите за YOLO на официальном сайте. 
Каждая версия была построена поверх предыдущей версии с улучшенными функциями, такими как улучшенная точность, более быстрая обработка и лучшая обработка небольших объектов.

YOLO широко используется в различных приложениях, таких как беспилотные автомобили и системы наблюдения. 
Он также широко используется для задач обнаружения объектов в реальном времени, таких как видеоаналитика в реальном времени и видеонаблюдение в реальном времени.

### Алгоритм YOLO: как он работает?

Основная идея YOLO заключается в том, чтобы разделить входное изображение на сетку ячеек и для каждой ячейки предсказать вероятность присутствия объекта и координаты ограничивающего прямоугольника объекта. Процесс YOLO можно разбить на несколько этапов:

Входное изображение пропускается через сверточную нейронную сеть для извлечения признаков из изображения.

Затем объекты пропускаются через ряд полносвязанных слоев, которые предсказывают вероятности классов и координаты ограничивающей рамки.

Изображение разделено на сетку ячеек, каждая из которых отвечает за прогнозирование набора ограничивающих рамок и вероятностей классов.

Выход сети представляет собой набор ограничивающих рамок и вероятностей классов для каждой ячейки.

Затем ограничивающие рамки фильтруются с использованием алгоритма постобработки, называемого немаксимальным подавлением, для удаления перекрывающихся рамок и выбора рамки с наибольшей вероятностью.

Конечный результат — набор прогнозируемых ограничивающих рамок и меток классов для каждого объекта на изображении.

Одним из ключевых преимуществ YOLO является то, что он обрабатывает все изображение за один проход, что делает его более быстрым и эффективным, чем двухступенчатые детекторы объектов, такие как R-CNN и его варианты.

![Архитектура YOLOv1](https://github.com/comedian73/Yolo_fish_detekt/blob/main/foto_yolo_/Yolo_architecture.png)
**Рис. 2. Архитектура YOLOv1**

*Cеть обнаружения объектов состоит из 24 сверточных слоев, за которыми следуют 2 полносвязанных слоя. 
Чередующиеся сверточные слои размером 1 × 1 уменьшают пространство объектов по сравнению с предыдущими слоями. 
Сверточные слои предварительно обучены в задаче классификации на датасете ImageNet с половинным разрешением (входное изображение 224 × 224), а для задачи обнаружения разрешение удваивается. 
Рисунок взят из оригинальной статьи.*

## Подготовка

В данной статье будет использоваться модель YOLOv11. Она будет дообучена на датасете с параметрами по умолчанию кроме минимально необходимых.

Так же обучим вторую модель с более широким списком параметров и гиперпараметров, и посмотрим как они отльчаются друг от друга. Распознаем объекты на видео и посчитаем сколько каждая модель обноружила объектов.

В качестве объектов для распознования будут рыбки. Да, те самые аквариумные жители которые считаются одними из лучших релоксаторами 🐟 🐟

Для работы с моделями, датасетами и общей логикой, необходимо установить и импортировать библиотеки.
```Python
# Установка пакета ultralytics для загрузки предобученных моделей YOLO
!pip install ultralytics
```
```Python
import ultralytics                    # пакета ultralytics для работы с YOLO
from ultralytics import YOLO          # непосредственно загрузчик моделей YOLO

from tqdm import tqdm                 # прогресс бар
import os                             # пакет для работы с операционной системой
import numpy as np                    # линейная алгебра
import cv2                            # мощная библиотека для работы с изображениями
import glob                           # библиотека для работы с дирикториями
import matplotlib.pyplot as plt       # визуализация, графики
from moviepy.editor import *          # библиотека для воспроизведения видео

import warnings                       # для работы с предупреждениями
warnings.filterwarnings('ignore')     # фильтрируем предупреждения

ultralytics.checks()                  # просмотр информации о среде
```
```Python
# Импортируем библиотеку для работы с Google Диском
# Чтобы сохранить и загружать файлы
from google.colab import drive
drive.mount('/content/drive')
```

Сразу определим вспомогательные функции.

Функция детекции обьектов была адаптированна из статьи на [Habr](https://habr.com/ru/articles/821971/) 

Так же подобная функция, в "легком" варианте, встречается на официальном сайте [Ultralytics](https://ultralytics.com)

```Python
# Функция детекции объектов в видео и возврат количество распознанных рыбок во всех кадрах
def detect_in_video(model, input, output):
    '''
    Определяет объекты на видео.
    @param model:  модель YOLO с помощью которой будет производится
                   детекция объектов/
    @param input:  строка указывающая путь к видеофайлу с расширение в котором
                   будет производится детекция.
                   Пример: '/content/drive/MyDrive/video.mp4'
    @param output: строка указывающая путь к обработаному видеофайлу с расширением.
                   Пример: 'detect.mp4'
    '''

    colors = [
      (255, 0, 0), (0, 255, 0)
      ]

    # Открытие исходного видеофайла
    capture = cv2.VideoCapture(input)

    # Получение общего числа кадров в видео
    frame_number = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))

    # Чтение параметров видео
    fps = int(capture.get(cv2.CAP_PROP_FPS))
    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Настройка выходного файла
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(output, fourcc, fps, (width, height))

    sum_fish = 0

    for i in tqdm(range(frame_number-1), ncols=80, ascii=True, desc='Обработка видео'):

        # Захват кадра
        ret, frame = capture.read()
        if not ret:
           break
        # Обработка кадра с помощью модели YOLO
        results = model(frame, verbose=False)[0]

        # Получение данных об объектах
        classes_names = results.names
        classes = results.boxes.cls.cpu().numpy()
        boxes = results.boxes.xyxy.cpu().numpy().astype(np.int32)

        # Рисование рамок и подписей на кадре
        for class_id, box, conf in zip(classes, boxes, results.boxes.conf):
            if conf>0.7: # если точность предсказания выше 70%
               class_name = classes_names[int(class_id)] +  " {:.2f}".format(float(conf))
               color = colors[int(class_id) % len(colors)]
               x1, y1, x2, y2 = box
               cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
               cv2.putText(frame, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

               sum_fish += len(boxes) # складываем количество распознаных рыбок в кадре

        # Запись обработанного кадра в выходной файл
        writer.write(frame)

    # Освобождение ресурсов и закрытие окон
    capture.release()
    writer.release()
    return 'Сумма распознаний на всех кадрах: ' + str(sum_fish)
```
```Puthon
# функция для вывода изображений и метрик
def result_train(path):
    img = cv2.imread(path)
    plt.figure(figsize=(8,8), dpi= 150)
    plt.imshow(img)
```

[Датасет](https://universe.roboflow.com/objectdetection-dwyrp/ornamental-fish-detection/dataset/4) был скачан с сайта [roboflow](https://roboflow.com/) и добавлен на [Google Drive](https://drive.google.com/file/d/11tTUNTSwWFQWRJSXXfso93qxDi2gLtM5/view?usp=drive_link)

```Python
# распаковываем архив датасета в папку fish
!unzip -d 'fish' -qo "/content/drive/MyDrive/fish.zip"

# определяем путь до файла конфигурации датасета
yaml_path = './fish/data.yaml'
```
